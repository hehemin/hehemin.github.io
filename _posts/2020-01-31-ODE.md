---
layout: post
title: Notes of Ordinary Differential Equations
use_math: true
tags: notes
desc: This blog article records what I learn in solving ODE.
---

<br>

# Introduction

## Definition and Classification

Briefly, a **differential equation** is a relationship among an unknown function and its derivatives.

Classification:

1. by the number of independent variables: ODE and PDE.

2. by the number of unknown functions: single function and system of differential equations.

3. by the order of unknown functions: the order of the highest derivative that appears in the equation.

4. by the relationship among unknown functions and their derivatives: Linear and Nonlinear Equations.

A **solution** of the ordinary differential equation on the interval is a function $\phi$ such that $\phi', \phi'',..., \phi^{n}$ exist and satisfy

$$
\phi^{n}(t) = f[t, \phi(t), \phi'(t),...,\phi^{n-1}(t)]
\tag{1}
$$

**General solution**  If the solution $y=\psi(t, C_1,...,C_n)$ of the differential equation

$$
y^{n} = f(t, y, y', y'',...,y^{n-1})
\tag{2}
$$

has $n$ independent constants $C_1,...,C_n$, then the solution is the **general solution** of the differential equation (2). The geometrical representation of the general solution is an infinite family of curves, called integral curves.

A **particular solution** of an ODE is a solution which does not include arbitrary constant.

A **singular solution** $y=\phi(t)$ of a first-order nonlinear ODE $\frac{dy}{dt}=f(t,y)$ is a solution that satisfies the followint two conditions:

1. $y=\phi(t)$ is not a specific case of the general solution;

2. For each given point on the curve $\Gamma$ of $y = \phi(t)$ there exists an integral curve in the one-parameter curve family which is determined by the general solution of the equation, and is tangent to $\Gamma$ at given point.
## Direction Fields

To draw a direction field, we first need to draw the isoclines of $\frac{dy}{dt} = f(y, t)$.    

# First Order Differential Equations 

$$
\frac{dy}{dt} = f(t,y)
$$

## Separable Equations

Notation: denote x as the independent variable. Our new problem is 

$$
\frac{dy}{dx} = f(x,y)
$$

Rewrite:

$$
M(x,y) + N(x,y) \frac{dy}{dx} = 0
$$

Specifically, in a **separable** form:

$$
M(x) + N(y) \frac{dy}{dx} = 0
$$

in which we can write in a differential form:

$$
M(x)dx + N(y)dy = 0
$$

Suppose $M(x) = H_1'(x)$, $N(y) = H_2'(y)$, and by chain rule,
$H_2'(y) \frac{dy}{dx} = \frac{dH_2(y)}{dx}$, we have

$$
\frac{d}{dx}[H_1(x)+H_2(y)] = 0
$$

Then $H_1(x) + H_2(y) = c$.
## Equations which can be Reduced to Separable Equations

- Type 1ï¼šHOD 0 Equations

By introducing a new variable $y = zx$

- Type 2: $\frac{dy}{dx} = f(ax + by + c)$

Let $z = ax + by + c$, then we get a seperable equation:

$$
\frac{dz}{dx} = a + b \frac{dy}{dx} = a + bf(z)
$$

- Type 3: $\frac{dy}{dx} = f(\frac{a_1x + b_1y + c_1}{a_2x + b_2y + c_2})$

when $c_1 \neq 0$ or $c_2 \neq 0$, two cases need be considered. First, the two lines $a_1x + b_1y + c_1 = 0$ and $a_2x + b_2y + c_2 = 0$ have an intersection point such as $(x_1, y_1)$. Let $\xi = x - x_1$, $\eta = y - y_1$, then we have 

$$
\frac{d\eta}{d\xi} = f(\frac{a_1\xi + b_1 \eta}{a_2\xi + b_2\eta})
$$
which is a homogeneous equation.

Second, if the two lines are parallel, i.e., $\frac{a_1}{a_2} = \frac{b_1}{b_2} = \lambda$. Let $z = a_2x + b_2y$, then 

$$
\frac{dz}{dx} = a_2 + b_2 \frac{dy}{dx} = a_2 + b_2 F(z)
$$

## Linear Equations 

$$
\frac{dy}{dt} + p(t)y = g(t)
$$

where $p$ and $g$ are given functions of the independent variable t.

Due to Laibniz, we use integration factor, $\mu(t)$.

Multiply equation by $\mu(t)$, we obtain

$$
\mu(t)\frac{dy}{dt} + p(t)\mu(t)y = \mu(t)g(t)
$$.

We see the left side is the derivative of the product $\mu(t)y$, provided that $\mu(t)$ satisfies the equation

$$
\frac{d\mu(t)}{dt} = p(t)\mu(t)
$$.

We assume temporarily that $\mu(t)$ is positive, then we have 

$$
\frac{d\mu(t)/dt}{\mu(t)} = p(t)
$$.

Consequently, 

$$
ln\mu(t) = \int p(t)dt + k
$$.

Choosing the arbitrary constant k to be zero, we obtain

$$
\mu(t) = exp \int p(t)dt
$$
Note that $\mu(t)$ is positive for all t, as we assumed. Return to the general form, we have

$$
\frac{d}{dt}[\mu(t)y] = \mu(t)g(t)
$$

Hence

$$
\mu(t)y = \int \mu(s)g(s)ds + c
$$
So the general solution is 

$$
y = \frac{\int \mu(s)g(s)ds + c}{\mu(t)}
$$


**Nonlinear first order ODE**

- Type 1: Bernoulli's Equation

$$
\frac{dy}{dx} + p(x)y = q(x)y^n
$$
where $n\neq0,1$. This equation may be rewritten as 

$$
y^{-n}\frac{dy}{dx} + p(x)y^{1-n} = q(x)
$$

Note that $\frac{d}{dx}y^{1-n} = (1-n)y^{-n}\frac{dy}{dx}$, so let $y^{1-n} = z$, we can rewrite Bernoulli form as 

$$
\frac{dz}{dx} + (1-n)p(x)z = (1-n)q(x)
$$
which is a linear differential equation.

- Type 2: Riccati's Equation

$$
\frac{dy}{dx} + p(x)y + q(x)y^2 = f(x)
$$
when $f(x)=0$, it's precisely the Bernolli's equation. Generally speaking, it's very difficult to solve the Riccati equation by the above methods. In some specific situation, we can reduce Ricatti equation into the Bernolli's equation. For example, if $y = y_1(x)$ is the particular solution of ricatti equation, then let $y = y_1 + z$, the Ricatti equation becomes 

$$
\frac{dz}{dx} + [p(x) + 2q(x)y_1]z + q(x)z^2 = 0
$$
because $\frac{dy_1}{dx} + p(x)y_1 + q(x)y_1^2 = f(x)$. 
## Exact Equations and Integrating Factors

Defination of **exact equation**: a class of equation where there is a well-defined method of solution.

Let

$$
M(x,y) + N(x,y)y' = 0

\tag{e}
$$

be a given differential equation. Suppose that we can identify a function $\psi$ such that 

$$
\frac{\partial \psi}{\partial x}(x,y) = M(x,y),
$$
and 

$$
\frac{\partial \psi}{\partial y}(x,y) = N(x,y), 
$$

and such that $\psi(x,y) = c$ defines $y = \phi(x)$ implicitly as a differential function of $x$. Then

$$
M(x,y) + N(x,y)y' = \frac{\partial \psi}{\partial x} + \frac{\partial \psi}{\partial y}\frac{dy}{dx} = \frac{d}{dx}\psi[x, \phi(x)]
$$

Then equation (e) becomes 

$$
\frac{d}{dx}\psi[x, \phi(x)] = 0.
$$

In this case, eq.(e) is called an exact differential equation. The solution is 

$$
\psi(x,y) = c,
$$

where $c$ is an arbitrary constant.

**Theorem** *Suppose that the functions $M$ and $N$, and their partial derivatives $M_y$ and $N_x$ are continuous in the rectangular region $R: \alpha<x<\beta, \gamma<y<\delta.$* *Then Eq.(e)*

$$
M(x,y) + N(x,y)y' = 0
$$

*is an exact differential equation in $R$ if and only if*

$$
M_y(x,y) = N_x(x,y)
$$

*at each point of $R$*. *That is, there exists a function $\psi$ satisfying*

$$
\frac{\partial \psi}{\partial x}(x,y) = M(x,y),
$$

*and* 

$$
\frac{\partial \psi}{\partial y}(x,y) = N(x,y), 
$$

*if and only if $M$ and $N$ satisfy* $M_y(x,y) = N_x(x,y)$.

**Integrating Factors**
It is sometimes possible to convert a differential equation that is not exact into an exact equation by multiplying the equation by a suitable integrating factor. Let's multiply the equation (e) by a function $\mu$ and then try to choose $\mu$ so that the resulting equation

$$
\mu(x,y)M(x,y)dx + \mu(x,y)N(x,y)dy = 0
$$

is exact. By theorem above, this equation is exact if and only if 

$$
(\mu M)_y = (\mu N)_x
$$

which means the integrating factor $\mu$ must satisfy the first order partial differential equation

$$
M \mu_y -N \mu_x + (M_y - N_x)\mu = 0.
$$

Unfortunately, this partial equation is usually as least as difficult to solve as the original equation. Therefore, in practice we like the situations that $\mu$ is a function of only one of the variable $x$ or $y$, instead of both. Let's determine the necessary conditions on $M$ and $N$ so that the original equation has an integrating factor $\mu$ that depends on $x$ only. (A similar procedure can be used to determine a condition that integrating factor depending only on $y$).

Assuming that $\mu$ is a function of $x$ only, we have 

$$
(\mu M)_y = \mu M_y,
$$

and 

$$
(\mu N)_x = \mu N_x + N \frac{d\mu}{dx}.
$$

Thus if $(\mu M)_y$ is euqal to $(\mu N)_x$, it is necessary that 

$$
\frac{d\mu}{dx} = \frac{M_y-N_x}{N}\mu.
$$
## Some special Nonlinear Equations

- Type 1: $y = f(x, y').$

Let $y'=p$ and take derivative w.r.t. $x$

$$
\frac{dy}{dx} = \frac{\partial f}{\partial x} + \frac{\partial f}{\partial p} \frac{dp}{dx}, 
$$

or

$$
p =  \frac{\partial f}{\partial x} + \frac{\partial f}{\partial p} \frac{dp}{dx},
$$

which is an explicitly first-order differential equation, and some method above can be used.

Remark: The equation 

$$
y = y'x + \psi(y')
$$

is called the *Clairaut equation*. The above methods can be used and get the general equation 

$$
y = cx + \psi(c),
$$

where $c$ is an arbitrary constant.
- Type 2: $x = f(y, y').$

Let $y'=p$ and take the derivative w.r.t y, so 

$$
x = f(y,p)
$$

and 

$$
\frac{1}{p} = \frac{dx}{dy} = \frac{\partial f}{\partial y} + \frac{\partial f}{\partial p} \frac{dp}{dy}.
$$
## Singular Solution

Consider an initial-value problem (IVP)

$$ 
\left\{
\begin{aligned}
\frac{dy}{dx} = f(x,y),\\
y(x_0) = y_0,
\end{aligned}
\right.
$$

which is defined over a rectangle $R = \{(x,y) | |x-x_0|\leq a, |y-y_0|\leq b\}$. The main assumptions in the following theorem are that $f$ is defined on $R$ and 

1) $f(x,y) \in \mathfrak{C}(R)$,
2) $f(x,y)$ satisfies a Lipschitz condition w.r.t $y$ in R: i.e., there exists a constant $L$ such that 

$$
|f(x, y_1) - f(x, y_2)| \leq L|y_1 - y_2|
$$

for any $(x, y_1),$ $(x,y_2) \in R$. The constant $L$ is called the **Lipschitz constant**.

**Existence and Uniqueness Theorem** *Suppose that $f\in \mathfrak{C}(R)$ satisfies the Lipschitz condition 2. Then the initial-value problem has exactly one solution in the intercal \[$x_0-h, x_0+h$\] with $h = min(a, \frac{b}{M})$, $M = \displaystyle\max_{(x,y) \in R} |f(x,y)|.$*


# Fundamental Theory

## The Existence and Uniqueness Theorem

## Extending Solutions and Continuous theorem

### An existence and uniqueness theorem under the local Lipschitz condition

### Extending Solutions

### Continuous Dependence on Initial Conditions

### Continuous Dependence on Parameters

# Second Order Linear Equations

$$
\frac{d^2y}{dt^2} = f(t, y, \frac{dy}{dt}),

\tag{s1}
$$

If the function $f$ has the form 

$$
f(t, y, \frac{dy}{dt})
= g(t) - p(t)\frac{dy}{dt} -q(t)y,
$$

that is  $f$ is linear in $y$ and $y'$, $g$, $p$, $q$ are specified functions of the independent variables $t$ but not depend on $y$.

In this case we usually rewrite eq.(s1) as 

$$
y'' + p(t)y' + q(t)y = g(t),

\tag{s2}
$$

where the primes denote differentiation w.r.t t.

Instead of eq.(s2) we often see the equation 

$$
P(t)y'' + Q(t)y' + R(t)y = G(t)
$$

Of course, if $P(t) \neq 0$ we can divide by $P(t)$ and thereby obtain

## Equations with Constant Coefficients: Single Real Roots

## Fundamental Theory for Homogeneous Equations

## Linear Independence and the Wroskian

## Equations with Constant Coefficients: Complex Roots

## Equations with Constant Coefficients: Repeated Roots

## Method of Variation of Parameters

## Methos of Undermined Coefficients

# Higher Order Linear Equations

## Geneal Theory of nth Order Linear Equations

## Homogeneous Equations with Constant Coefficients

## The Method of Undermined Coefficients

## The Method of Variation of Parameters

# Systems of First Order Linear Equations

## Basic Theory of Linear Systems

## Single Real Eigenvalues

## Complex Eigenvalues

## Fundamental Matrices

## Repeated Eigenvalues

## Nonhomogeneous Linear Systems

## First Integral

# Qualitative Theory of Differential Equations

## Autonomous Systems and Stability

## Some Concepts for Planar Autonomous Systems

## Phase Portraits for Planar Linear Systems

### Real Distinct Eigenvalues

### Complex Eigenvalues

### Repeated Eigenvalues 

### The Trace-Determinant Plane

## Almost Linear Systems

## Liapunov's Direct Method



